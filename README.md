# Домашнее Задание №3
<br>
<p>Чтобы развернуть airflow, предварительно собрав контейнеры</p>

```
#Собирать в images/airflow-ml-base
docker build -t airflow-ml-base:latest .
```

<p>Для корректной работы с переменными, созданными из UI</p>

```
export FERNET_KEY=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")
docker compose up --build
```

<br>

Легенда:
1) Откуда-то берутся данные... Мы их используем для обучения МЛ модельки для задачи классификации.
2) Еженедельно, мы переобучаем модель на новых данных, ручками смотрим на метрики и если класс, то выкатываем ее на прод.
3) Ежедневно, текущая выбранная нами модель скорит данные и записывает предсказания куда-то.
4) Эти предсказания используют -- все счастливы =)

В ДЗ предлагается на основе `airflow` реализовать описанную выше схему, к деталям:

**Основная часть:**

- [x] Поднимите airflow локально, используя `docker compose` (можно использовать из примера https://github.com/made-ml-in-prod-2021/airflow-examples/)
- [] Реализуйте dag, который генерирует данные для обучения модели (генерируйте данные -- можете использовать как генератор синтетики из первой дз, так и что-то из датасетов sklearn). Вам важно проэмулировать ситуации постоянно поступающих данных (5 баллов)

    - записывайте данные в `/data/raw/{{ ds }}/data.csv` и `/data/raw/{{ ds }}/target.csv`

- [x] Реализуйте dag, который обучает модель еженедельно, используя данные за текущий день. В вашем пайплайне должно быть как минимум 4 стадии, но дайте волю своей фантазии =) (10 баллов)

    - подготовить данные для обучения (например, считать из `/data/raw/{{ ds }}` и положить `/data/processed/{{ ds }}/train_data.csv`)
    - расплитить их на train/val
    - обучить модель на train, сохранить в `/data/models/{{ ds }}`
    - провалидировать модель на val (сохранить метрики к модельке)

- [x] Реализуйте dag, который использует модель ежедневно (5 баллов)
    - принимает на вход данные из пункта 1 (`data.csv`)
    - считывает путь до модельки из airflow variables (идея в том, что когда нам нравится другая модель и мы хотим ее на прод)
    - делает предсказание и записывает их в `/data/predictions/{{ ds }}/predictions.csv`

- [x] Все даги реализованы только с помощью `DockerOperator` (пример    https://github.com/made-ml-in-prod-2021/airflow-examples/blob/main/dags/11_docker.py). По технике, вы можете использовать такую же структуру как в примере, пакуя в разные докеры скрипты, можете использовать общий докер с вашим пакетом, но с разными точками входа для разных тасок. Прикольно, если вы покажете, что для разных тасок можно использовать разный набор зависимостей (10 баллов)

- [x] Традиционно, самооценка (1 балл)

**Дополнительная часть**:

- [x] Реализуйте сенсоры на то, что данные готовы для дагов тренировки и обучения (+3 доп балла)
